{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import lit\n","\n","# 1. Define the list of CSV files and their corresponding staging table names.\n","#    We will use a simple naming convention: Stg_{Table_Name}\n","files_to_process = {\n","    \"olist_customers_dataset.csv\": \"Stg_Customers\",\n","    \"olist_orders_dataset.csv\": \"Stg_Orders\",\n","    \"olist_order_items_dataset.csv\": \"Stg_OrderItems\",\n","    \"olist_order_payments_dataset.csv\": \"Stg_OrderPayments\",\n","    \"olist_products_dataset.csv\": \"Stg_Products\",\n","    \"olist_sellers_dataset.csv\": \"Stg_Sellers\",\n","    \"olist_geolocation_dataset.csv\": \"Stg_Geolocation\",\n","    \"olist_order_reviews_dataset.csv\": \"Stg_OrderReviews\",\n","    \"product_category_name_translation.csv\": \"Stg_ProductCategoryTranslation\"\n","}\n","\n","# Define a single record source for all tables\n","record_source = \"Kaggle_Olist\"\n","\n","# 2. Loop through each file, read it into a DataFrame, and save it as a table.\n","for file_name, table_name in files_to_process.items():\n","    print(f\"Processing file: {file_name} -> Saving to table: {table_name}\")\n","    try:\n","        # Construct the file path within the Lakehouse 'Files' section\n","        file_path = f\"Files/{file_name}\"\n","\n","        # Read the CSV file into a DataFrame\n","        # The 'header' option is set to true because the first row contains column names.\n","        df_raw = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_path)\n","\n","        # Add the 'record_source' column to the DataFrame.\n","        # This is a key part of Data Vault for auditing and traceability.\n","        df_staged = df_raw.withColumn(\"Record_Source\", lit(record_source))\n","\n","        # Write the DataFrame to a Delta table in the Lakehouse 'Tables' section.\n","        # 'overwrite' mode ensures that if the table exists, it's recreated.\n","        df_staged.write.mode(\"overwrite\").format(\"delta\").saveAsTable(table_name)\n","        \n","        print(f\"Successfully saved {table_name}.\")\n","    except Exception as e:\n","        print(f\"Error processing {file_name}: {e}\")\n","        # Continue to the next file even if one fails.\n","\n","print(\"\\nAll files have been processed. You can now see the tables in your Lakehouse.\")\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"eaea3dc9-14e9-4fab-a49a-ebe3ee7f9852","normalized_state":"finished","queued_time":"2025-08-12T13:54:04.7240941Z","session_start_time":null,"execution_start_time":"2025-08-12T13:54:04.7254239Z","execution_finish_time":"2025-08-12T13:54:47.4705453Z","parent_msg_id":"6ff6e4ca-5481-487f-bcb1-f717167c97c0"},"text/plain":"StatementMeta(, eaea3dc9-14e9-4fab-a49a-ebe3ee7f9852, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Processing file: olist_customers_dataset.csv -> Saving to table: Stg_Customers\nError processing olist_customers_dataset.csv: [_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: 490c751b-0342-4c92-b769-b4818a74c3bd).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- customer_id: string (nullable = true)\n-- customer_unique_id: string (nullable = true)\n-- customer_zip_code_prefix: string (nullable = true)\n-- customer_city: string (nullable = true)\n-- customer_state: string (nullable = true)\n\n\nData schema:\nroot\n-- customer_id: string (nullable = true)\n-- customer_unique_id: string (nullable = true)\n-- customer_zip_code_prefix: string (nullable = true)\n-- customer_city: string (nullable = true)\n-- customer_state: string (nullable = true)\n-- Record_Source: string (nullable = true)\n\n         \nTo overwrite your schema or change partitioning, please set:\n'.option(\"overwriteSchema\", \"true\")'.\n\nNote that the schema can't be overwritten when using\n'replaceWhere'.\n         \nProcessing file: olist_orders_dataset.csv -> Saving to table: Stg_Orders\nSuccessfully saved Stg_Orders.\nProcessing file: olist_order_items_dataset.csv -> Saving to table: Stg_OrderItems\nSuccessfully saved Stg_OrderItems.\nProcessing file: olist_order_payments_dataset.csv -> Saving to table: Stg_OrderPayments\nSuccessfully saved Stg_OrderPayments.\nProcessing file: olist_products_dataset.csv -> Saving to table: Stg_Products\nSuccessfully saved Stg_Products.\nProcessing file: olist_sellers_dataset.csv -> Saving to table: Stg_Sellers\nSuccessfully saved Stg_Sellers.\nProcessing file: olist_geolocation_dataset.csv -> Saving to table: Stg_Geolocation\nSuccessfully saved Stg_Geolocation.\nProcessing file: olist_order_reviews_dataset.csv -> Saving to table: Stg_OrderReviews\nSuccessfully saved Stg_OrderReviews.\nProcessing file: product_category_name_translation.csv -> Saving to table: Stg_ProductCategoryTranslation\nSuccessfully saved Stg_ProductCategoryTranslation.\n\nAll files have been processed. You can now see the tables in your Lakehouse.\n"]}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"advisor":{"adviceMetadata":"{\"artifactId\":\"fe1edf25-fd99-45c1-86e4-685ff5548f0d\",\"activityId\":\"eaea3dc9-14e9-4fab-a49a-ebe3ee7f9852\",\"applicationId\":\"application_1755006223956_0001\",\"jobGroupId\":\"8\",\"advices\":{\"info\":1}}"}},"id":"2e61dbcc-a282-4c3c-abe0-9a87d2068dc7"},{"cell_type":"code","source":["# This script creates all the Hub tables for the Data Vault model from the staged data.\n","# Hubs represent the core business entities of the e-commerce dataset.\n","# The script uses a consistent pattern to generate a unique hash key and add metadata\n","# for each hub, which is essential for data traceability and consistency.\n","\n","from pyspark.sql.functions import sha2, lit, current_timestamp\n","\n","# Define a single record source for all hub tables\n","record_source = \"Kaggle_Olist\"\n","\n","# --- Create Hub_Customer ---\n","print(\"Creating Hub_Customer table...\")\n","df_stg_customers = spark.read.table(\"Stg_Customers\")\n","df_hub_customer = df_stg_customers.select(\n","    sha2(\"customer_unique_id\", 256).alias(\"Customer_HKEY\"),\n","    \"customer_unique_id\",\n","    lit(current_timestamp()).alias(\"Load_Date\"),\n","    lit(record_source).alias(\"Record_Source\")\n",").distinct()\n","df_hub_customer.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Hub_Customer\")\n","print(\"Hub_Customer created successfully.\")\n","\n","# --- Create Hub_Order ---\n","print(\"Creating Hub_Order table...\")\n","df_stg_orders = spark.read.table(\"Stg_Orders\")\n","df_hub_order = df_stg_orders.select(\n","    sha2(\"order_id\", 256).alias(\"Order_HKEY\"),\n","    \"order_id\",\n","    lit(current_timestamp()).alias(\"Load_Date\"),\n","    lit(record_source).alias(\"Record_Source\")\n",").distinct()\n","df_hub_order.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Hub_Order\")\n","print(\"Hub_Order created successfully.\")\n","\n","# --- Create Hub_Product ---\n","print(\"Creating Hub_Product table...\")\n","# Note: olist_products_dataset.csv contains information about products\n","df_stg_products = spark.read.table(\"Stg_Products\")\n","df_hub_product = df_stg_products.select(\n","    sha2(\"product_id\", 256).alias(\"Product_HKEY\"),\n","    \"product_id\",\n","    lit(current_timestamp()).alias(\"Load_Date\"),\n","    lit(record_source).alias(\"Record_Source\")\n",").distinct()\n","df_hub_product.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Hub_Product\")\n","print(\"Hub_Product created successfully.\")\n","\n","# --- Create Hub_Seller ---\n","print(\"Creating Hub_Seller table...\")\n","# Note: olist_sellers_dataset.csv contains information about sellers\n","df_stg_sellers = spark.read.table(\"Stg_Sellers\")\n","df_hub_seller = df_stg_sellers.select(\n","    sha2(\"seller_id\", 256).alias(\"Seller_HKEY\"),\n","    \"seller_id\",\n","    lit(current_timestamp()).alias(\"Load_Date\"),\n","    lit(record_source).alias(\"Record_Source\")\n",").distinct()\n","df_hub_seller.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Hub_Seller\")\n","print(\"Hub_Seller created successfully.\")\n","\n","print(\"\\nAll hub tables have been created and are now available in your Lakehouse.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"eaea3dc9-14e9-4fab-a49a-ebe3ee7f9852","normalized_state":"finished","queued_time":"2025-08-12T14:05:30.3494839Z","session_start_time":null,"execution_start_time":"2025-08-12T14:05:30.3507628Z","execution_finish_time":"2025-08-12T14:05:53.3132639Z","parent_msg_id":"3ea93756-8eb5-4944-8d22-be776343c1ad"},"text/plain":"StatementMeta(, eaea3dc9-14e9-4fab-a49a-ebe3ee7f9852, 11, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Creating Hub_Customer table...\nHub_Customer created successfully.\nCreating Hub_Order table...\nHub_Order created successfully.\nCreating Hub_Product table...\nHub_Product created successfully.\nCreating Hub_Seller table...\nHub_Seller created successfully.\n\nAll hub tables have been created and are now available in your Lakehouse.\n"]}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f2cedb98-d954-4b74-b508-8b7bff3cef12"},{"cell_type":"code","source":["# This script creates all the Satellite tables for the Data Vault model from the staged data.\n","# Satellites hold descriptive attributes and track the history of changes for each hub.\n","# The code joins the raw data with the corresponding hub table to link the attributes to the\n","# correct hash key and adds auditing metadata.\n","\n","from pyspark.sql.functions import lit, current_timestamp\n","\n","# Define a single record source for all satellite tables\n","record_source = \"Kaggle_Olist\"\n","\n","# --- Create Sat_Customer_Details ---\n","print(\"Creating Sat_Customer_Details table...\")\n","df_stg_customers = spark.read.table(\"Stg_Customers\")\n","df_hub_customer = spark.read.table(\"Hub_Customer\")\n","\n","df_sat_customer = df_stg_customers.join(df_hub_customer, on=\"customer_unique_id\").select(\n","    \"Customer_HKEY\",\n","    \"customer_city\",\n","    \"customer_state\",\n","    \"customer_zip_code_prefix\",\n","    lit(current_timestamp()).alias(\"Load_Date\"),\n","    lit(record_source).alias(\"Record_Source\")\n",")\n","df_sat_customer.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Sat_Customer_Details\")\n","print(\"Sat_Customer_Details created successfully.\")\n","\n","# --- Create Sat_Product_Details ---\n","print(\"Creating Sat_Product_Details table...\")\n","df_stg_products = spark.read.table(\"Stg_Products\")\n","df_hub_product = spark.read.table(\"Hub_Product\")\n","\n","df_sat_product = df_stg_products.join(df_hub_product, on=\"product_id\").select(\n","    \"Product_HKEY\",\n","    \"product_category_name\",\n","    \"product_name_lenght\",\n","    \"product_description_lenght\",\n","    \"product_photos_qty\",\n","    \"product_weight_g\",\n","    \"product_length_cm\",\n","    \"product_height_cm\",\n","    \"product_width_cm\",\n","    lit(current_timestamp()).alias(\"Load_Date\"),\n","    lit(record_source).alias(\"Record_Source\")\n",")\n","df_sat_product.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Sat_Product_Details\")\n","print(\"Sat_Product_Details created successfully.\")\n","\n","# --- Create Sat_Seller_Details ---\n","print(\"Creating Sat_Seller_Details table...\")\n","df_stg_sellers = spark.read.table(\"Stg_Sellers\")\n","df_hub_seller = spark.read.table(\"Hub_Seller\")\n","\n","df_sat_seller = df_stg_sellers.join(df_hub_seller, on=\"seller_id\").select(\n","    \"Seller_HKEY\",\n","    \"seller_zip_code_prefix\",\n","    \"seller_city\",\n","    \"seller_state\",\n","    lit(current_timestamp()).alias(\"Load_Date\"),\n","    lit(record_source).alias(\"Record_Source\")\n",")\n","df_sat_seller.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Sat_Seller_Details\")\n","print(\"Sat_Seller_Details created successfully.\")\n","\n","# --- Create Sat_Order_Details ---\n","print(\"Creating Sat_Order_Details table...\")\n","df_stg_orders = spark.read.table(\"Stg_Orders\")\n","df_hub_order = spark.read.table(\"Hub_Order\")\n","\n","df_sat_order = df_stg_orders.join(df_hub_order, on=\"order_id\").select(\n","    \"Order_HKEY\",\n","    \"customer_id\", # customer_id here links to the order, not the unique customer\n","    \"order_status\",\n","    \"order_purchase_timestamp\",\n","    \"order_approved_at\",\n","    \"order_delivered_carrier_date\",\n","    \"order_delivered_customer_date\",\n","    \"order_estimated_delivery_date\",\n","    lit(current_timestamp()).alias(\"Load_Date\"),\n","    lit(record_source).alias(\"Record_Source\")\n",")\n","df_sat_order.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Sat_Order_Details\")\n","print(\"Sat_Order_Details created successfully.\")\n","\n","# --- Create Sat_Order_Reviews ---\n","print(\"Creating Sat_Order_Reviews table...\")\n","df_stg_reviews = spark.read.table(\"Stg_OrderReviews\")\n","df_hub_order = spark.read.table(\"Hub_Order\")\n","\n","df_sat_reviews = df_stg_reviews.join(df_hub_order, on=\"order_id\").select(\n","    \"Order_HKEY\",\n","    \"review_id\",\n","    \"review_score\",\n","    \"review_comment_title\",\n","    \"review_comment_message\",\n","    \"review_creation_date\",\n","    \"review_answer_timestamp\",\n","    lit(current_timestamp()).alias(\"Load_Date\"),\n","    lit(record_source).alias(\"Record_Source\")\n",")\n","df_sat_reviews.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Sat_Order_Reviews\")\n","print(\"Sat_Order_Reviews created successfully.\")\n","\n","print(\"\\nAll satellite tables have been created and are now available in your Lakehouse.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"eaea3dc9-14e9-4fab-a49a-ebe3ee7f9852","normalized_state":"finished","queued_time":"2025-08-12T14:11:20.4530695Z","session_start_time":null,"execution_start_time":"2025-08-12T14:11:20.4541864Z","execution_finish_time":"2025-08-12T14:11:54.3471299Z","parent_msg_id":"d2db00ae-6cc9-4f4e-b7d6-86751c333f9e"},"text/plain":"StatementMeta(, eaea3dc9-14e9-4fab-a49a-ebe3ee7f9852, 12, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Creating Sat_Customer_Details table...\nSat_Customer_Details created successfully.\nCreating Sat_Product_Details table...\nSat_Product_Details created successfully.\nCreating Sat_Seller_Details table...\nSat_Seller_Details created successfully.\nCreating Sat_Order_Details table...\nSat_Order_Details created successfully.\nCreating Sat_Order_Reviews table...\nSat_Order_Reviews created successfully.\n\nAll satellite tables have been created and are now available in your Lakehouse.\n"]}],"execution_count":10,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"11522f64-9cbb-403d-805b-4695c75657b2"},{"cell_type":"code","source":["# This script creates the Link and Link Satellite tables for the Data Vault model.\n","# Links model the relationships between hubs, while link satellites store descriptive\n","# attributes about those relationships.\n","\n","from pyspark.sql.functions import sha2, lit, current_timestamp, concat_ws\n","\n","# Define a single record source for all tables\n","record_source = \"Kaggle_Olist\"\n","\n","# --- Read all necessary Hub and Staging tables ---\n","# We will read all the tables needed to build the links in one go.\n","print(\"Reading necessary Hub and Staging tables...\")\n","df_hub_customer = spark.read.table(\"Hub_Customer\")\n","df_hub_order = spark.read.table(\"Hub_Order\")\n","df_hub_product = spark.read.table(\"Hub_Product\")\n","df_hub_seller = spark.read.table(\"Hub_Seller\")\n","\n","df_stg_orders = spark.read.table(\"Stg_Orders\")\n","df_stg_customers = spark.read.table(\"Stg_Customers\")\n","df_stg_order_items = spark.read.table(\"Stg_OrderItems\")\n","df_stg_order_reviews = spark.read.table(\"Stg_OrderReviews\")\n","\n","\n","# --- Create Link_Customer_Order ---\n","# This link connects a unique customer to an order they placed.\n","# We first join Stg_Orders with Stg_Customers to get the customer_unique_id.\n","print(\"Creating Link_Customer_Order...\")\n","df_link_customer_order = df_stg_orders.join(df_stg_customers, on=\"customer_id\") \\\n","                                     .join(df_hub_customer, on=\"customer_unique_id\") \\\n","                                     .join(df_hub_order, on=\"order_id\") \\\n","                                     .select(\n","                                        sha2(concat_ws(\"||\", \"customer_unique_id\", \"order_id\"), 256).alias(\"Customer_Order_HKEY\"),\n","                                        \"Customer_HKEY\",\n","                                        \"Order_HKEY\",\n","                                        lit(current_timestamp()).alias(\"Load_Date\"),\n","                                        lit(record_source).alias(\"Record_Source\")\n","                                     ).distinct()\n","df_link_customer_order.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Link_Customer_Order\")\n","print(\"Link_Customer_Order created successfully.\")\n","\n","\n","# --- Create Link_Order_Review ---\n","# This link connects an order to its review. The original code was correct.\n","print(\"Creating Link_Order_Review...\")\n","df_link_order_review = df_stg_order_reviews.join(df_hub_order, on=\"order_id\") \\\n","                                            .select(\n","                                                sha2(\"order_id\", 256).alias(\"Order_Review_HKEY\"),\n","                                                \"Order_HKEY\",\n","                                                lit(current_timestamp()).alias(\"Load_Date\"),\n","                                                lit(record_source).alias(\"Record_Source\")\n","                                            ).distinct()\n","df_link_order_review.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Link_Order_Review\")\n","print(\"Link_Order_Review created successfully.\")\n","\n","\n","# --- Create Link_OrderItem_Product_Seller (and its satellite) ---\n","# This is a more complex link that connects an item from an order to a product and a seller.\n","print(\"Creating Link_OrderItem_Product_Seller and its satellite...\")\n","\n","# Step 1: Create a temporary DataFrame with all necessary keys and attributes.\n","# The 'df_joined_item_data' DataFrame will hold all the columns needed for both the link and its satellite.\n","df_joined_item_data = df_stg_order_items.join(df_hub_order, on=\"order_id\") \\\n","                                 .join(df_hub_product, on=\"product_id\") \\\n","                                 .join(df_hub_seller, on=\"seller_id\") \\\n","                                 .withColumn(\"OrderItem_Product_Seller_HKEY\",\n","                                             sha2(concat_ws(\"||\", \"order_id\", \"product_id\", \"seller_id\"), 256))\n","\n","# Step 2: Create the Link table from the temporary DataFrame.\n","# The link table only contains the hash keys and metadata.\n","df_link_item = df_joined_item_data.select(\n","    \"OrderItem_Product_Seller_HKEY\",\n","    \"Order_HKEY\",\n","    \"Product_HKEY\",\n","    \"Seller_HKEY\",\n","    lit(current_timestamp()).alias(\"Load_Date\"),\n","    lit(record_source).alias(\"Record_Source\")\n",").distinct()\n","\n","df_link_item.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Link_OrderItem_Product_Seller\")\n","print(\"Link_OrderItem_Product_Seller created successfully.\")\n","\n","# Step 3: Create the Link Satellite table from the temporary DataFrame.\n","# The satellite table contains the link's hash key and the descriptive attributes.\n","df_sat_link_item = df_joined_item_data.select(\n","    \"OrderItem_Product_Seller_HKEY\",\n","    \"order_id\",\n","    \"order_item_id\",\n","    \"shipping_limit_date\",\n","    \"price\",\n","    \"freight_value\",\n","    lit(current_timestamp()).alias(\"Load_Date\"),\n","    lit(record_source).alias(\"Record_Source\")\n",").distinct()\n","\n","df_sat_link_item.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Sat_Link_OrderItem_Product_Seller\")\n","print(\"Sat_Link_OrderItem_Product_Seller created successfully.\")\n","\n","print(\"\\nAll link tables and their satellites have been created and are now available in your Lakehouse.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":15,"statement_ids":[15],"state":"finished","livy_statement_state":"available","session_id":"eaea3dc9-14e9-4fab-a49a-ebe3ee7f9852","normalized_state":"finished","queued_time":"2025-08-12T14:17:30.2213942Z","session_start_time":null,"execution_start_time":"2025-08-12T14:17:30.2229213Z","execution_finish_time":"2025-08-12T14:18:03.0190917Z","parent_msg_id":"296248bb-ed1a-400f-a2e7-bc979e5150c0"},"text/plain":"StatementMeta(, eaea3dc9-14e9-4fab-a49a-ebe3ee7f9852, 15, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Reading necessary Hub and Staging tables...\nCreating Link_Customer_Order...\nLink_Customer_Order created successfully.\nCreating Link_Order_Review...\nLink_Order_Review created successfully.\nCreating Link_OrderItem_Product_Seller and its satellite...\nLink_OrderItem_Product_Seller created successfully.\nSat_Link_OrderItem_Product_Seller created successfully.\n\nAll link tables and their satellites have been created and are now available in your Lakehouse.\n"]}],"execution_count":13,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0acba534-b27e-4c65-835e-a889794879e2"},{"cell_type":"code","source":["# This script creates a corrected Information Mart for customer spending.\n","# It now properly joins the Data Vault tables by connecting through the Link table\n","# that bridges the relationship between orders and their item details.\n","\n","from pyspark.sql.functions import sum, desc\n","\n","# --- Read the necessary Data Vault tables ---\n","# We need information about customers, orders, and the items in those orders.\n","print(\"Reading Data Vault tables...\")\n","df_hub_customer = spark.read.table(\"Hub_Customer\")\n","df_link_customer_order = spark.read.table(\"Link_Customer_Order\")\n","# We need the Link table to bridge the relationship\n","df_link_order_item = spark.read.table(\"Link_OrderItem_Product_Seller\")\n","df_sat_link_item = spark.read.table(\"Sat_Link_OrderItem_Product_Seller\")\n","\n","\n","# --- Join the tables to get all the necessary data ---\n","# 1. Join Link_Customer_Order with the new Link_OrderItem_Product_Seller table on Order_HKEY.\n","#    This connects the customer to each specific item within their order.\n","df_combined = df_link_customer_order.join(df_link_order_item, on=\"Order_HKEY\")\n","\n","# 2. Join the result with the satellite (Sat_Link_OrderItem_Product_Seller) on its hash key.\n","#    This pulls in the price and freight value for each item.\n","df_final = df_combined.join(df_sat_link_item, on=\"OrderItem_Product_Seller_HKEY\")\n","\n","# 3. Join with Hub_Customer to get the customer's unique ID.\n","df_final_with_customer = df_final.join(df_hub_customer, on=\"Customer_HKEY\")\n","\n","# --- Aggregate the data to create the Information Mart ---\n","# Group by the unique customer ID and sum the total price and freight value.\n","print(\"Aggregating customer spending data...\")\n","df_customer_spending_mart = df_final_with_customer.groupBy(\"customer_unique_id\").agg(\n","    sum(\"price\").alias(\"total_price\"),\n","    sum(\"freight_value\").alias(\"total_freight_value\"),\n","    (sum(\"price\") + sum(\"freight_value\")).alias(\"total_spending\")\n",").orderBy(desc(\"total_spending\"))\n","\n","# --- Save the Information Mart to the Lakehouse ---\n","# The final table is denormalized and ready for direct consumption by a reporting tool.\n","print(\"Saving Customer Spending Information Mart...\")\n","df_customer_spending_mart.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Mart_Customer_Spending\")\n","print(\"Mart_Customer_Spending created successfully.\")\n","\n","print(\"\\nInformation Mart created. The data is now ready for analysis!\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":17,"statement_ids":[17],"state":"finished","livy_statement_state":"available","session_id":"eaea3dc9-14e9-4fab-a49a-ebe3ee7f9852","normalized_state":"finished","queued_time":"2025-08-12T14:37:34.4659077Z","session_start_time":null,"execution_start_time":"2025-08-12T14:37:34.4674203Z","execution_finish_time":"2025-08-12T14:37:51.3273952Z","parent_msg_id":"3be02cca-24af-4eda-af51-56547b06d518"},"text/plain":"StatementMeta(, eaea3dc9-14e9-4fab-a49a-ebe3ee7f9852, 17, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Reading Data Vault tables...\nAggregating customer spending data...\nSaving Customer Spending Information Mart...\nMart_Customer_Spending created successfully.\n\nInformation Mart created. The data is now ready for analysis!\n"]}],"execution_count":15,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"74ecdc6b-348d-49dd-8395-ca68109561b2"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"ac0db7ad-8ede-4618-9cf9-479fe6d617a1"}],"default_lakehouse":"ac0db7ad-8ede-4618-9cf9-479fe6d617a1","default_lakehouse_name":"project","default_lakehouse_workspace_id":"a8579c13-b20f-420a-b95b-16a5c6d70479"}}},"nbformat":4,"nbformat_minor":5}